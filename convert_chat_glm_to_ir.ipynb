{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4fc7366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./local_chat_glm_6b_model\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7682aa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcbba6d7ed894a2f8cd3cda683d5e9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = AutoModel.from_pretrained(\"./local_chat_glm_6b_model\", trust_remote_code=True).half().cuda(1)\n",
    "# model = AutoModel.from_pretrained(\"./local_chat_glm_6b_model\", trust_remote_code=True).cuda()\n",
    "model = AutoModel.from_pretrained(\"./local_chat_glm_6b_model\", trust_remote_code=True).to(\"cpu\", dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9447eebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGLMModel(\n",
       "  (word_embeddings): Embedding(130528, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-27): 28 x GLMBlock(\n",
       "      (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): SelfAttention(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "        (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      )\n",
       "      (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GLU(\n",
       "        (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "# response, history = model.chat(tokenizer, \"‰Ω†Â•Ω\", history=[])\n",
    "# print(response)\n",
    "\n",
    "# model\n",
    "# model.transformer.num_layers\n",
    "# print(model.transformer)\n",
    "# model.transformer.eval()\n",
    "model.transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10d64d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# response, history = model.chat(tokenizer, \"‰Ω†Â•Ω\", history=[])\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae7c79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 4])\n",
      "inputs_embeds= None\n",
      "input_ids.shape= torch.Size([1, 4])\n",
      "batch_size= tensor(1)\n",
      "seq_length= tensor(4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiping_dev/.cache/huggingface/modules/transformers_modules/local_chat_glm_6b_model/modeling_chatglm.py:685: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  context_lengths = [seq.tolist().index(self.config.bos_token_id) for seq in input_ids]\n",
      "/home/xiping_dev/.cache/huggingface/modules/transformers_modules/local_chat_glm_6b_model/modeling_chatglm.py:685: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  context_lengths = [seq.tolist().index(self.config.bos_token_id) for seq in input_ids]\n",
      "/home/xiping_dev/.cache/huggingface/modules/transformers_modules/local_chat_glm_6b_model/modeling_chatglm.py:949: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  seqs = input_ids.tolist()\n",
      "/home/xiping_dev/.cache/huggingface/modules/transformers_modules/local_chat_glm_6b_model/modeling_chatglm.py:699: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  context_lengths = [seq.tolist().index(self.config.bos_token_id) for seq in input_ids]\n",
      "/home/xiping_dev/.cache/huggingface/modules/transformers_modules/local_chat_glm_6b_model/modeling_chatglm.py:699: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  context_lengths = [seq.tolist().index(self.config.bos_token_id) for seq in input_ids]\n",
      "/home/xiping_dev/.cache/huggingface/modules/transformers_modules/local_chat_glm_6b_model/modeling_chatglm.py:1005: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  layer_id=torch.tensor(i),\n",
      "/home/xiping_dev/.cache/huggingface/modules/transformers_modules/local_chat_glm_6b_model/modeling_chatglm.py:267: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  query_key_layer_scaling_coeff = float(layer_id + 1)\n",
      "/home/xiping_dev/.cache/huggingface/modules/transformers_modules/local_chat_glm_6b_model/modeling_chatglm.py:269: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  query_layer = query_layer / (math.sqrt(hidden_size) * query_key_layer_scaling_coeff)\n",
      "/home/xiping_dev/.cache/huggingface/modules/transformers_modules/local_chat_glm_6b_model/modeling_chatglm.py:304: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not (attention_mask == 0).all():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Text Encoder successfully converted to ONNX\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# model.transformer.layers\n",
    "# type(model.transformer.word_embeddings)\n",
    "# type(model.transformer)\n",
    "# model.transformer.eval()\n",
    "\n",
    "import numpy as np\n",
    "input_ids = torch.tensor(np.array([[5, 74874, 130001, 130004]]), dtype=torch.long)\n",
    "\n",
    "# switch model to inference mode\n",
    "model.transformer.eval()\n",
    "onnx_path = \"./output_onnx\"\n",
    "\n",
    "# disable gradients calculation for reducing memory consumption\n",
    "with torch.no_grad():\n",
    "    # export model to ONNX format\n",
    "    torch.onnx.export(\n",
    "        model.transformer,  # model instance\n",
    "        input_ids,  # inputs for model tracing\n",
    "        onnx_path,  # output file for saving result\n",
    "        input_names=['tokens'],  # model input name for onnx representation\n",
    "        output_names=['last_hidden_state', 'pooler_out'],  # model output names for onnx representation\n",
    "    )\n",
    "print('Text Encoder successfully converted to ONNX')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c32e1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The dtype of attention mask (torch.int64) is not bool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs= {'input_ids': tensor([[     5,  74874, 130001, 130004]])}\n",
      "self.device= cpu\n",
      "inputs[input_ids]= tensor([[     5,  74874, 130001, 130004]])\n",
      "input_ids.shape= torch.Size([1, 4])\n",
      "batch_size= 1\n",
      "seq_length= 4\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 4])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "------------------------forword--->\n",
      "input_ids.shape= torch.Size([1, 1])\n",
      "inputs_embeds= None\n",
      "‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\n",
      "CPU times: user 10min 27s, sys: 1min 17s, total: 11min 45s\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response, history = model.chat(tokenizer, \"‰Ω†Â•Ω\", history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59cdf17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m transformers.onnx --model=\"./local_chat_glm_6b_model\" onnx/ --framework pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bed176",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = \"./output_onnx\"\n",
    "import onnx\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e441c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# ov_model_dir = Path(\"ov_model_dir\")\n",
    "# ov_model_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eda369d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:30\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# import gc\n",
    "# import torch\n",
    "\n",
    "# MODEL_1_ONNX_PATH = ov_model_dir / 'model_1.onnx'\n",
    "# MODEL_1_OV_PATH = MODEL_1_ONNX_PATH.with_suffix('.xml')\n",
    "\n",
    "# def convert_model_1_onnx(model_1: torch.nn.Module, onnx_path:Path):\n",
    "    \n",
    "#     if not onnx_path.exists():\n",
    "#         input_ids = torch.ones((1, 77), dtype=torch.long)\n",
    "#         # switch model to inference mode\n",
    "#         model_1.eval()\n",
    "\n",
    "#         # disable gradients calculation for reducing memory consumption\n",
    "#         with torch.no_grad():\n",
    "#             # export model to ONNX format\n",
    "#             torch.onnx._export(\n",
    "#                 text_encoder,  # model instance\n",
    "#                 input_ids,  # inputs for model tracing\n",
    "#                 onnx_path,  # output file for saving result\n",
    "#                 input_names=['tokens'],  # model input name for onnx representation\n",
    "#                 output_names=['last_hidden_state', 'pooler_out'],  # model output names for onnx representation\n",
    "#                 opset_version=14,  # onnx opset version for export,\n",
    "#                 onnx_shape_inference=False\n",
    "#             )\n",
    "#         print('Text Encoder successfully converted to ONNX')\n",
    "\n",
    "\n",
    "# if not MODEL_1_OV_PATH.exists():\n",
    "#     convert_model_1_onnx(text_encoder, MODEL_1_ONNX_PATH)\n",
    "#     !mo --input_model $TEXT_ENCODER_ONNX_PATH --compress_to_fp16 --output_dir $sd2_1_model_dir\n",
    "#     print('Text Encoder successfully converted to IR')\n",
    "# else:\n",
    "#     print(f\"Text encoder will be loaded from {TEXT_ENCODER_OV_PATH}\")\n",
    "\n",
    "# del text_encoder\n",
    "\n",
    "# gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b21563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
